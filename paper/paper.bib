@misc{botnanlesnick,
  title = {An {{Introduction}} to {{Multiparameter Persistence}}},
  author = {Botnan, Magnus Bakke and Lesnick, Michael},
  year = {2022},
  month = mar,
  number = {arXiv:2203.14289},
  eprint = {2203.14289},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.14289},
  urldate = {2022-06-11},
  abstract = {In topological data analysis (TDA), one often studies the shape of data by
              constructing a filtered topological space, whose structure is then examined using
              persistent homology. However, a single filtered space often does not adequately capture
              the structure of interest in the data, and one is led to consider multiparameter
              persistence, which associates to the data a space equipped with a multiparameter
              filtration. Multiparameter persistence has become one of the most active areas of
              research within TDA, with exciting progress on several fronts. In this article, we
              introduce multiparameter persistence and survey some of this recent progress, with a
              focus on ideas likely to lead to practical applications in the near future.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Mathematics
              - Representation Theory},
}

@article{code_persistable,
  title = {Persistable: Persistent and Stable Clustering},
  shorttitle = {Persistable},
  author = {Scoccola, Luis and Rolle, Alexander},
  year = {2023},
  month = mar,
  journal = {Journal of Open Source Software},
  volume = {8},
  number = {83},
  pages = {5022},
  issn = {2475-9066},
  doi = {10.21105/joss.05022},
  urldate = {2024-05-09},
  abstract = {Scoccola et al., (2023). Persistable: persistent and stable clustering. Journal of
              Open Source Software, 8(83), 5022, https://doi.org/10.21105/joss.05022},
  langid = {english},
}

@article{erstaircode,
  title = {Elder-{{Rule-Staircodes}} for {{Augmented Metric Spaces}}},
  author = {Cai, Chen and Kim, Woojin and Memoli, Facundo and Wang, Yusu},
  year = {2021},
  month = jan,
  journal = {SIAM Journal on Applied Algebra and Geometry},
  volume = {5},
  number = {3},
  eprint = {2003.04523},
  primaryclass = {cs, math},
  pages = {417--454},
  issn = {2470-6566},
  doi = {10.1137/20M1353605},
  urldate = {2022-05-20},
  abstract = {An augmented metric space is a metric space \$(X, d\_X)\$ equipped with a function \$f
              \_X: X {\textbackslash}to {\textbackslash}mathbb\{R\}\$. This type of data arises
              commonly in practice, e.g, a point cloud \$X\$ in \${\textbackslash}mathbb\{R\}{
              \textasciicircum}d\$ where each point \$x{\textbackslash}in X\$ has a density function
              value \$f\_X(x)\$ associated to it. An augmented metric space \$(X, d\_X, f\_X)\$
              naturally gives rise to a 2-parameter filtration \${\textbackslash}mathcal\{K\}\$.
              However, the resulting 2-parameter persistent homology \${\textbackslash}mathrm\{H\}\_
              \{{\textbackslash}bullet\}({\textbackslash}mathcal\{K\})\$ could still be of wild
              representation type, and may not have simple indecomposables. In this paper, motivated
              by the elder-rule for the zeroth homology of 1-parameter filtration, we propose a
              barcode-like summary, called the elder-rule-staircode, as a way to encode \${
              \textbackslash}mathrm\{H\}\_0({\textbackslash}mathcal\{K\})\$. Specifically, if \$n = {
              \textbar}X{\textbar}\$, the elder-rule-staircode consists of \$n\$ number of
              staircase-like blocks in the plane. We show that if \${\textbackslash}mathrm\{H\}\_0({
              \textbackslash}mathcal\{K\})\$ is interval decomposable, then the barcode of \${
              \textbackslash}mathrm\{H\}\_0({\textbackslash}mathcal\{K\})\$ is equal to the
              elder-rule-staircode. Furthermore, regardless of the interval decomposability, the
              fibered barcode, the dimension function (a.k.a. the Hilbert function), and the graded
              Betti numbers of \${\textbackslash}mathrm\{H\}\_0({\textbackslash}mathcal\{K\})\$ can
              all be efficiently computed once the elder-rule-staircode is given. Finally, we develop
              and implement an efficient algorithm to compute the elder-rule-staircode in \$O(n{
              \textasciicircum}2{\textbackslash}log n)\$ time, which can be improved to \$O(n{
              \textasciicircum}2{\textbackslash}alpha(n))\$ if \$X\$ is from a fixed dimensional
              Euclidean space \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$, where \${
              \textbackslash}alpha(n)\$ is the inverse Ackermann function.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology},
}

@article{eulearning,
  title = {Euler {{Characteristic Tools For Topological Data Analysis}}},
  author = {Hacquard, Olympio and Lebovici, Vadim},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  doi = {10.48550/arxiv.2303.14040},
  urldate = {2024-04-16},
  abstract = {In this article, we study Euler characteristic techniques in topological data
              analysis. Pointwise computing the Euler characteristic of a family of simplicial
              complexes built from data gives rise to the so-called Euler characteristic profile. We
              show that this simple descriptor achieve state-of-the-art performance in supervised
              tasks at a very low computational cost. Inspired by signal analysis, we compute hybrid
              transforms of Euler characteristic profiles. These integral transforms mix Euler
              characteristic techniques with Lebesgue integration to provide highly efficient
              compressors of topological signals. As a consequence, they show remarkable performances
              in unsupervised settings. On the qualitative side, we provide numerous heuristics on
              the topological and geometric information captured by Euler profiles and their hybrid
              transforms. Finally, we prove stability results for these descriptors as well as
              asymptotic guarantees in random settings.},
  langid = {english},
}

@incollection{filtration_domination,
  title = {Filtration-Domination in Bifiltered Graphs},
  booktitle = {2023 Proceedings of the Symposium on Algorithm Engineering and Experiments ({{ALENEX}
               })},
  author = {Alonso, {\'A}ngel Javier and Kerber, Michael and Pritam, Siddharth},
  year = {2023},
  eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977561.ch3},
  pages = {27--38},
  doi = {10.1137/1.9781611977561.ch3},
  abstract = {Bifiltered graphs are a versatile tool for modelling relations between data points
              across multiple grades of a two- dimensional scale. They are especially popular in
              topological data analysis, where the homological properties of the induced clique
              complexes are studied. To reduce the large size of these clique complexes, we identify
              filtration-dominated edges of the graph, whose removal preserves the relevant
              topological properties. We give two algorithms to detect filtration-dominated edges in
              a bifiltered graph and analyze their complexity. These two algorithms work directly on
              the bifiltered graph, without first extracting the clique complexes, which are
              generally much bigger. We present extensive experimental evaluation which shows that in
              most cases, more than 90\% of the edges can be removed. In turn, we demonstrate that
              this often leads to a substantial speedup, and reduction in the memory usage, of the
              computational pipeline of multiparameter topological data analysis.},
}

@incollection{function_delaunay,
  title = {Delaunay {{Bifiltrations}} of {{Functions}} on {{Point Clouds}}},
  booktitle = {Proceedings of the 2024 {{Annual ACM-SIAM Symposium}} on {{Discrete Algorithms}} ({{
               SODA}})},
  author = {Alonso, {\'A}ngel Javier and Kerber, Michael and Lam, Tung and Lesnick, Michael},
  year = {2024},
  month = jan,
  series = {Proceedings},
  pages = {4872--4891},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611977912.173},
  urldate = {2024-05-14},
  abstract = {The Delaunay filtration D.(X) of a point cloud X {$\subset$} {$\mathbb{R}$}d is a
              central tool of computational topology. Its use is justified by the topological
              equivalence of D. (X) and the offset (i.e., union-of-balls) filtration of X. Given a
              function {$\gamma$} : X {$\rightarrow$} {$\mathbb{R}$}, we introduce a Delaunay
              bifiltration DC.({$\gamma$}) that satisfies an analogous topological equivalence,
              ensuring that DC. ({$\gamma$}) topologically encodes the offset filtrations of all
              sublevel sets of {$\gamma$}, as well as the topological relations between them. DC.({$
              \gamma$}) is of size , which for d odd matches the worst-case size of D. (X). Adapting
              the Bowyer-Watson algorithm for computing Delaunay triangulations, we give a simple,
              practical algorithm to compute DC.({$\gamma$}) in time Our implementation, based on
              CGAL, computes DC. ({$\gamma$}) with modest overhead compared to computing D. (X), and
              handles tens of thousands of points in {$\mathbb{R}$}3 within seconds.},
}

@inproceedings{gril,
  title = {{{GRIL}}: {{A}} \$2\$-Parameter {{Persistence Based Vectorization}} for {{Machine
           Learning}}},
  shorttitle = {{{GRIL}}},
  booktitle = {Proceedings of 2nd {{Annual Workshop}} on {{Topology}}, {{Algebra}}, and {{Geometry}}
               in {{Machine Learning}} ({{TAG-ML}})},
  author = {Xin, Cheng and Mukherjee, Soham and Samaga, Shreyas N. and Dey, Tamal K.},
  year = {2023},
  month = sep,
  pages = {313--333},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-16},
  abstract = {111-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA),
              studies the evolution of topological features such as connected components and cycles
              hidden in data. It has been applied to enhance the representation power of deep
              learning models, such as Graph Neural Networks (GNNs). To enrich the representations of
              topological features, here we propose to study 222-parameter persistence modules
              induced by bi-filtration functions. In order to incorporate these representations into
              machine learning models, we introduce a novel vector representation called Generalized
              Rank Invariant Landscape (GRIL) for 222-parameter persistence modules. We show that
              this vector representation is 111-Lipschitz stable and differentiable with respect to
              underlying filtration functions and can be easily integrated into machine learning
              models to augment encoding topological features. We present an algorithm to compute the
              vector representation efficiently. We also test our methods on synthetic and benchmark
              graph datasets, and compare the results with previous vector representations of
              111-parameter and 222-parameter persistence modules. Further, we augment GNNs with GRIL
              features and observe an increase in performance indicating that GRIL can capture
              additional features enriching GNNs. We make the complete code for the proposed method
              available at
              [https://github.com/soham0209/mpml-graph](https://github.com/soham0209/mpml-graph).},
  langid = {english},
}

@misc{gudhi,
  title = {{{GUDHI}}},
  author = {TheGudhiProject},
  year = {2023},
  howpublished = {GUDHI Editorial Board},
}

@inproceedings{mariaGudhiLibrarySimplicial2014,
  title = {The {{Gudhi Library}}: {{Simplicial Complexes}} and {{Persistent Homology}}},
  shorttitle = {The {{Gudhi Library}}},
  booktitle = {Mathematical {{Software}} -- {{ICMS}} 2014},
  author = {Maria, Cl{\'e}ment and Boissonnat, Jean-Daniel and Glisse, Marc and Yvinec, Mariette},
  editor = {Hong, Hoon and Yap, Chee},
  year = {2014},
  pages = {167--174},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-44199-2_28},
  abstract = {We present the main algorithmic and design choices that have been made to represent
              complexes and compute persistent homology in the Gudhi library. The Gudhi library
              (Geometric Understanding in Higher Dimensions) is a generic C++ library for
              computational topology. Its goal is to provide robust, efficient, flexible and easy to
              use implementations of state-of-the-art algorithms and data structures for
              computational topology. We present the different components of the software, their
              interaction and the user interface. We justify the algorithmic and design decisions
              made in Gudhi and provide benchmarks for the code. The software, which has been
              developped by the first author, will be available soon at
              project.inria.fr/gudhi/software/.},
  isbn = {978-3-662-44199-2},
  langid = {english},
}

@misc{mma,
  title = {Fast, {{Stable}} and {{Efficient Approximation}} of {{Multi-parameter Persistence Modules
           }} with {{MMA}}},
  author = {Loiseaux, David and Carri{\`e}re, Mathieu and Blumberg, Andrew J.},
  year = {2022},
  month = jun,
  doi = {10.48550/arXiv.2206.02026},
  urldate = {2024-04-16},
  abstract = {In this article, we introduce a new parameterized family of topological invariants,
              taking the form of candidate decompositions, for multi-parameter persistence modules.
              We prove that our candidate decompositions are controllable approximations: when
              restricting to modules that can be decomposed into interval summands, we establish
              theoretical results about the approximation error between our candidate decompositions
              and the true underlying module in terms of the standard interleaving and bottleneck
              distances. Moreover, even when the underlying module does not admit such a
              decomposition, our candidate decompositions are nonetheless stable invariants; small
              perturbations in the underlying module lead to small perturbations in the candidate
              decomposition. Then, we introduce MMA (Multipersistence Module Approximation): an
              algorithm for computing stable instances of such invariants, which is based on fibered
              barcodes and exact matchings, two constructions that stem from the theory of
              single-parameter persistence. By design, MMA can handle an arbitrary number of
              filtrations, and has bounded complexity and running time. Finally, we present empirical
              evidence validating the generalization capabilities and running time speed-ups of MMA
              on several data sets.},
  langid = {english},
}

@article{mma_vect,
  title = {A {{Framework}} for {{Fast}} and {{Stable Representations}} of {{Multiparameter
           Persistent Homology Decompositions}}},
  author = {Loiseaux, David and Carri{\`e}re, Mathieu and Blumberg, Andrew},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {35774--35798},
  urldate = {2024-03-19},
  langid = {english},
  keywords = {No DOI found},
}

@article{mpfree,
  title = {Fast {{Minimal Presentations}} of {{Bi-graded Persistence Modules}}},
  author = {Kerber, Michael and Rolle, Alexander},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.15623 [cs, math]},
  eprint = {2010.15623},
  primaryclass = {cs, math},
  doi = {10.1137/1.9781611976472.16},
  urldate = {2021-07-21},
  abstract = {Multi-parameter persistent homology is a recent branch of topological data analysis.
              In this area, data sets are investigated through the lens of homology with respect to
              two or more scale parameters. The high computational cost of many algorithms calls for
              a preprocessing step to reduce the input size. In general, a minimal presentation is
              the smallest possible representation of a persistence module. Lesnick and Wright [28]
              proposed recently an algorithm (the LW-algorithm) for computing minimal presentations
              based on matrix reduction.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {55N99 13D02,Computer Science - Symbolic Computation,Mathematics - Algebraic Topology,
              Mathematics - Commutative Algebra,No DOI found},
}

@article{mpl,
  title = {Multiparameter Persistence Landscapes},
  author = {Vipond, Oliver},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  pages = {61:1--61:38},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  keywords = {No DOI found},
  timestamp = {Thu, 18 Jun 2020 22:13:22 +0200},
}

@article{persistable,
  title = {Stable and Consistent Density-Based Clustering via Multiparameter Persistence},
  author = {Rolle, Alexander and Scoccola, Luis},
  year = {2020},
  month = may,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2005.09048},
  urldate = {2024-04-16},
  abstract = {We consider the degree-Rips construction from topological data analysis, which
              provides a density-sensitive, multiparameter hierarchical clustering algorithm. We
              analyze its stability to perturbations of the input data using the
              correspondence-interleaving distance, a metric for hierarchical clusterings that we
              introduce. Taking certain one-parameter slices of degree-Rips recovers well-known
              methods for density-based clustering, but we show that these methods are unstable.
              However, we prove that degree-Rips, as a multiparameter object, is stable, and we
              propose an alternative approach for taking slices of degree-Rips, which yields a
              one-parameter hierarchical clustering algorithm with better stability properties. We
              prove that this algorithm is consistent, using the correspondence-interleaving
              distance. We provide an algorithm for extracting a single clustering from one-parameter
              hierarchical clusterings, which is stable with respect to the
              correspondence-interleaving distance. And, we integrate these methods into a pipeline
              for density-based clustering, which we call Persistable. Adapting tools from
              multiparameter persistent homology, we propose visualization tools that guide the
              selection of all parameters of the pipeline. We demonstrate Persistable on benchmark
              datasets, showing that it identifies multi-scale cluster structure in data.},
  langid = {english},
}

@article{pot,
  title = {{{POT}}: {{Python}} Optimal Transport},
  shorttitle = {{{POT}}},
  author = {Flamary, R{\'e}mi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z. and
            Boisbunon, Aur{\'e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien
            and Fatras, Kilian and Fournier, Nemo and Gautheron, L{\'e}o and Gayraud, Nathalie T.H.
            and Janati, Hicham and Rakotomamonjy, Alain and Redko, Ievgen and Rolet, Antoine and
            Schutz, Antony and Seguy, Vivien and Sutherland, Danica J. and Tavenard, Romain and Tong,
            Alexander and Vayer, Titouan},
  year = {2021},
  month = jan,
  journal = {The Journal of Machine Learning Research},
  volume = {22},
  number = {1},
  pages = {78:3571--78:3578},
  issn = {1532-4435},
  abstract = {Optimal transport has recently been reintroduced to the machine learning community
              thanks in part to novel efficient optimization procedures allowing for medium to large
              scale applications. We propose a Python toolbox that implements several key optimal
              transport ideas for the machine learning community. The toolbox contains
              implementations of a number of founding works of OT for machine learning such as
              Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that
              can be used for conducting novel fundamental research. This toolbox, named POT for
              Python Optimal Transport, is open source with an MIT license.},
  keywords = {divergence,domain adaptation,No DOI found,optimal transport,optimization},
}

@article{pykeops,
  title = {Kernel {{Operations}} on the {{GPU}}, with {{Autodiff}}, without {{Memory Overflows}}},
  author = {Charlier, Benjamin and Feydy, Jean and Glaun{\`e}s, Joan Alexis and Collin, Fran{\c c}
            ois-David and Durif, Ghislain},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {74},
  pages = {1--6},
  issn = {1533-7928},
  urldate = {2024-04-16},
  abstract = {The KeOps library provides a fast and memory-efficient GPU support for tensors whose
              entries are given by a mathematical formula, such as kernel and distance matrices.
              KeOps alleviates the main bottleneck of tensor-centric libraries for kernel and
              geometric applications: memory consumption. It also supports automatic differentiation
              and outperforms standard GPU baselines, including PyTorch CUDA tensors or the Halide
              and TVM libraries. KeOps combines optimized C++/CUDA schemes with binders for
              high-level languages: Python (Numpy and PyTorch), Matlab and GNU R. As a result,
              high-level ``quadratic'' codes can now scale up to large data sets with millions of
              samples processed in seconds. KeOps brings graphics-like performances for kernel
              methods and is freely available on standard repositories (PyPi, CRAN). To showcase its
              versatility, we provide tutorials in a wide range of settings online at
              www.kernel-operations.io.},
  keywords = {No DOI found},
}

@incollection{pytorch,
  title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning Library},
  shorttitle = {{{PyTorch}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information
               Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and
            Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga,
            Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and
            Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang,
            Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {721},
  pages = {8026--8037},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-04-16},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not
              both. PyTorch is a machine learning library that shows that these two goals are in fact
              compatible: it provides an imperative and Pythonic programming style that supports code
              as a model, makes debugging easy and is consistent with other popular scientific
              computing libraries, while remaining efficient and supporting hardware accelerators
              such as GPUs. In this paper, we detail the principles that drove the implementation of
              PyTorch and how they are reflected in its architecture. We emphasize that every aspect
              of PyTorch is a regular Python program under the full control of its user. We also
              explain how the careful and pragmatic implementation of the key components of its
              runtime enables them to work together to achieve compelling performance. We demonstrate
              the efficiency of individual subsystems, as well as the overall speed of PyTorch on
              several common benchmarks.},
}

@article{rivet,
  title = {Interactive Visualization of 2-{{D}} Persistence Modules},
  author = {Lesnick, Michael and Wright, Matthew},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.00180 [cs, math]},
  eprint = {1512.00180},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.1512.00180},
  abstract = {The goal of this work is to extend the standard persistent homology pipeline for
              exploratory data analysis to the 2-D persistence setting, in a practical,
              computationally efficient way. To this end, we introduce RIVET, a software tool for the
              visualization of 2-D persistence modules, and present mathematical foundations for this
              tool. RIVET provides an interactive visualization of the barcodes of 1-D affine slices
              of a 2-D persistence module M . It also computes and visualizes the dimension of each
              vector space in M and the bigraded Betti numbers of M . At the heart of our
              computational approach is a novel data structure based on planar line arrangements, on
              which we can perform fast queries to find the barcode of any slice of M . We present an
              efficient algorithm for constructing this data structure and establish bounds on its
              complexity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Mathematics
              - Commutative Algebra,No DOI found,yrf2022},
}

@article{sb_as_sm,
  title = {Stable {{Vectorization}} of {{Multiparameter Persistent Homology}} Using {{Signed
           Barcodes}} as {{Measures}}},
  author = {Loiseaux, David and Scoccola, Luis and Carri{\`e}re, Mathieu and Botnan, Magnus Bakke
            and Oudot, Steve},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {68316--68342},
  urldate = {2024-03-19},
  langid = {english},
  keywords = {No DOI found},
}

@article{scikit_learn,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent
            and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter
            and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and
            Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  urldate = {2024-04-16},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine
              learning algorithms for medium-scale supervised and unsupervised problems. This package
              focuses on bringing machine learning to non-specialists using a general-purpose
              high-level language. Emphasis is put on ease of use, performance, documentation, and
              API consistency. It has minimal dependencies and is distributed under the simplified
              BSD license, encouraging its use in both academic and commercial settings. Source code,
              binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.
              },
  keywords = {No DOI found},
}

@inproceedings{signed_barcode,
  title = {Signed {{Barcodes}} for {{Multi-Parameter Persistence}} via {{Rank Decompositions}}},
  booktitle = {38th {{International Symposium}} on {{Computational Geometry}} ({{SoCG}} 2022)},
  author = {Botnan, Magnus Bakke and Oppermann, Steffen and Oudot, Steve},
  editor = {Goaoc, Xavier and Kerber, Michael},
  year = {2022},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {224},
  pages = {19:1--19:18},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address = {Dagstuhl, Germany},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.SoCG.2022.19},
  urldate = {2022-06-14},
  isbn = {978-3-95977-227-3},
  keywords = {multi-parameter persistent homology,Topological data analysis},
}

@article{signed_betti,
  title = {On the {{Stability}} of {{Multigraded Betti Numbers}} and {{Hilbert Functions}}},
  author = {Oudot, Steve and Scoccola, Luis},
  year = {2024},
  month = mar,
  journal = {SIAM Journal on Applied Algebra and Geometry},
  volume = {8},
  number = {1},
  pages = {54--88},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/22M1489150},
  urldate = {2024-05-14},
  abstract = {.We introduce harmonic persistent homology spaces for filtrations of finite simplicial
              complexes. As a result, we can associate concrete subspaces of cycles to each bar of
              the barcode of the filtration. We prove stability of the harmonic persistent homology
              subspaces as well as the subspaces associated to the bars of the barcodes under small
              perturbations of functions defining them. We relate the notion of ``essential
              simplices'' introduced in an earlier work to identify simplices which play a
              significant role in the birth of a bar with that of harmonic persistent homology. We
              prove that the harmonic representatives of simple bars maximize the ``relative
              essential content'' among all representatives of the bar, where the relative essential
              content is the weight a particular cycle puts on the set of essential simplices.},
}

@inproceedings{sm_diff,
  title = {Differentiability and {{Optimization}} of {{Multiparameter Persistent Homology}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Scoccola, Luis and Setlur, Siddharth and Loiseaux, David and Carri{\`e}re, Mathieu and
            Oudot, Steve},
  year = {2024},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {235},
  pages = {43986--44011},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-10-02},
  abstract = {Real-valued functions on geometric data---such as node attributes on a graph---can be
              optimized using descriptors from persistent homology, allowing the user to incorporate
              topological terms in the loss function. When optimizing a single real-valued function
              (the one-parameter setting), there is a canonical choice of descriptor for persistent
              homology: the barcode. The operation mapping a real-valued function to its barcode is
              differentiable almost everywhere, and the convergence of gradient descent for losses
              using barcodes is relatively well understood. When optimizing a vector-valued function
              (the multiparameter setting), there is no unique choice of descriptor for
              multiparameter persistent homology, and many distinct descriptors have been proposed.
              This calls for the development of a general framework for differentiability and
              optimization that applies to a wide range of multiparameter homological descriptors. In
              this article, we develop such a framework and show that it encompasses well-known
              descriptors of different flavors, such as signed barcodes and the multiparameter
              persistence landscape. We complement the theory with numerical experiments supporting
              the idea that optimizing multiparameter homological descriptors can lead to improved
              performances compared to optimizing one-parameter descriptors, even when using the
              simplest and most efficiently computable multiparameter descriptors.},
  langid = {english},
}

@misc{pytest,
  title = {Pytest 8.3},
  author = {Krekel, Holger and Oliveira, Bruno and Pfannschmidt, Ronny and Bruynooghe, Floris and
            Laugher, Brianna and Bruhin, Florian},
  year = {2004},
}



@article{cython,
  title = {Cython: {{The}} Best of Both Worlds},
  author = {Behnel, S. and Bradshaw, R. and Citro, C. and Dalcin, L. and Seljebotn, D.S. and Smith,
            K.},
  year = {2011-03/2011-04},
  journal = {Computing in Science Engineering},
  volume = {13},
  number = {2},
  pages = {31--39},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2010.118},
  keywords = {C language,Cython language,Fortran code,numerical analysis,numerical loops,programming
              language,Python language extension},
}

@incollection{tbb,
  title = {{{Intel}}{\textregistered} {{Threading Building Blocks}} ({{TBB}})},
  booktitle = {Encyclopedia of {{Parallel Computing}}},
  author = {Robison, Arch D.},
  editor = {Padua, David},
  year = {2011},
  pages = {955--964},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-09766-4_51},
  urldate = {2024-10-09},
  isbn = {978-0-387-09766-4},
  langid = {english},
}
